# üéâ SUCCESS! Datapizza RAG Stack is Running

## ‚úÖ All Services Operational

### Service Status

| Service | Port | Status | URL |
|---------|------|--------|-----|
| **VittoriaDB** | 8080 | ‚úÖ Healthy | http://localhost:8080 |
| **Backend API** | 8501 | ‚úÖ Healthy | http://localhost:8501 |
| **Frontend UI** | 3000 | ‚úÖ Running | http://localhost:3000 |

## üöÄ Access Your Application

### Main Application
**Open in browser**: http://localhost:3000

### API Documentation
**Backend Swagger Docs**: http://localhost:8501/docs

### Health Checks
```bash
# VittoriaDB
curl http://localhost:8080/health

# Backend
curl http://localhost:8501/health
```

## üéØ What You Can Do Now

### 1. Start Chatting
- Go to http://localhost:3000
- Type a question in the chat
- Get AI-powered responses!

### 2. Upload Documents
1. Click "üìÑ Documents" in sidebar
2. Drag & drop or select files (PDF, DOCX, TXT, MD)
3. Wait for embedding generation
4. Ask questions about your documents!

### 3. Web Research
1. Click "üåê Web Research"
2. Enter a topic or question
3. System will:
   - Search the web
   - Extract and index content
   - Answer your questions

### 4. Index GitHub Repos
1. Click "üì¶ GitHub"
2. Enter repository URL
3. System will index the code
4. Ask questions about the codebase

## üìä System Configuration

### Embeddings (Datapizza AI Compatible)
```
Provider: openai
Model: text-embedding-3-small
Dimensions: 1536
Status: ‚úÖ Active
```

### Collections
- `documents`: üìÑ User uploads
- `web_research`: üåê Web content
- `github_code`: üíª Code repositories  
- `chat_history`: üí¨ Conversations
- `advanced_rag_kb`: üß† Advanced RAG

### Current Stats
- **Total Vectors**: 87
- **Collections**: 9
- **Uptime**: VittoriaDB running since startup

## üîß Managing Services

### View Logs
```bash
# Backend logs
tail -f /tmp/datapizza-backend.log

# Frontend logs (in terminal where it's running)
cd examples/web-ui-rag/frontend
# Check the terminal output
```

### Stop Services
```bash
# Stop all services
lsof -ti:8501 | xargs kill -9  # Backend
lsof -ti:8080 | xargs kill -9  # VittoriaDB  
lsof -ti:3000 | xargs kill -9  # Frontend
```

### Restart Services
```bash
# Terminal 1: VittoriaDB
cd /Users/d695663/Desktop/Dev/CognitoraVector
./build/vittoriadb run --port 8080

# Terminal 2: Backend
cd examples/datapizza-rag/backend
source venv/bin/activate
python main.py

# Terminal 3: Frontend
cd examples/web-ui-rag/frontend
npm run dev
```

## üé® Configuration Options

### Switch to Ollama (Local Embeddings)

1. Install Ollama:
```bash
brew install ollama
ollama serve
ollama pull nomic-embed-text
```

2. Update backend/.env:
```bash
EMBEDDER_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_EMBED_DIMENSIONS=768
```

3. Restart backend:
```bash
lsof -ti:8501 | xargs kill -9
cd examples/datapizza-rag/backend
source venv/bin/activate
python main.py
```

### Adjust RAG Parameters

Edit `backend/.env`:
```bash
# Chunk size for documents
CHUNK_SIZE=1000

# Overlap between chunks
CHUNK_OVERLAP=200

# Minimum similarity score
MIN_SIMILARITY_SCORE=0.3

# Number of results to return
SEARCH_LIMIT=5
```

## üìö Documentation

- **Quick Start**: [QUICK_START.md](./QUICK_START.md)
- **Full README**: [README.md](./README.md)
- **Datapizza Integration**: [backend/DATAPIZZA_INTEGRATION.md](./backend/DATAPIZZA_INTEGRATION.md)
- **Status Report**: [STATUS.md](./STATUS.md)

## üéØ Example Queries

Try these in the chat:

### General Questions
```
"What is VittoriaDB?"
"How do I use embeddings?"
"Explain RAG systems"
```

### Document Questions (after uploading)
```
"Summarize the main points of the uploaded document"
"What are the key findings?"
"List all the topics mentioned"
```

### Web Research
```
"What are the latest AI developments?"
"Research machine learning trends 2025"
"Find information about vector databases"
```

### GitHub Code Search (after indexing)
```
"Show me the authentication code"
"How is the database configured?"
"Explain the API endpoints"
```

## üî• Performance Tips

### Faster Embeddings
- Use OpenAI (cloud) for speed: ~0.15s/doc
- Use Ollama (local) with GPU: ~0.3s/doc
- Batch upload multiple documents

### Better Search Results
- Use descriptive filenames
- Add metadata to uploads
- Ask specific questions
- Adjust `min_score` threshold

### Optimize Memory
- Clear unused collections
- Limit search results
- Use smaller embedding models for testing

## üêõ Common Issues

### Port Already in Use
```bash
lsof -ti:8501 | xargs kill -9
```

### Backend Won't Start
```bash
# Check VittoriaDB
curl http://localhost:8080/health

# Check environment
cat backend/.env

# View logs
tail -f /tmp/datapizza-backend.log
```

### Frontend Shows Errors
```bash
# Ensure backend is running
curl http://localhost:8501/health

# Check frontend env
cat frontend/.env.local
```

## üéä You're All Set!

The Datapizza RAG stack is fully operational with:

‚úÖ **VittoriaDB** - High-performance vector database  
‚úÖ **Backend** - FastAPI with datapizza-ai embeddings  
‚úÖ **Frontend** - Modern ChatGPT-like UI  
‚úÖ **Embeddings** - OpenAI-compatible (supports Ollama)  
‚úÖ **RAG** - Production-ready retrieval system  

**Start using it now**: http://localhost:3000

---

**Need help?**
- Check [QUICK_START.md](./QUICK_START.md)
- Review [STATUS.md](./STATUS.md)
- Read [backend/DATAPIZZA_INTEGRATION.md](./backend/DATAPIZZA_INTEGRATION.md)

**Built with:**
- [Datapizza AI](https://github.com/datapizza-labs/datapizza-ai) - AI Framework
- [VittoriaDB](https://vittoriadb.com) - Vector Database
- [Next.js](https://nextjs.org) - Frontend
- [FastAPI](https://fastapi.tiangolo.com) - Backend

üçï Enjoy your Datapizza RAG stack!

